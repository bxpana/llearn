import { Lesson } from "@/lib/types";

export const scoringCriteria: Lesson = {
  slug: "scoring-criteria",
  title: "Scoring Criteria and Rubrics",
  description:
    "Design rubrics with binary, categorical, and component scoring to measure output quality consistently.",
  order: 3,
  content: {
    explanation: `Scoring criteria define what "good" means for your eval. Without a clear rubric, two reviewers will score the same output differently, and your eval results become unreliable.

There are three main scoring approaches:

**Binary scoring** — pass/fail. Best for tasks with objectively correct answers (classification, extraction, yes/no questions). Simple and unambiguous.

**Categorical scoring** — a scale like 1-3 or 1-5. Best for generation tasks where quality exists on a spectrum. The trap: without anchor descriptions, everyone interprets the scale differently. A "3" should mean the same thing to every scorer.

**Component scoring** — multiple dimensions scored independently. For example, scoring a summary on accuracy, completeness, and conciseness separately. This is the most informative approach because it tells you *which aspect* is failing.

Each approach can use either **deterministic scoring** (regex, exact match, string comparison) or **LLM-as-judge** (a second LLM evaluates the output). Deterministic scoring is more reliable but limited to structured outputs. LLM-as-judge handles open-ended generation but introduces its own biases.

The key to any rubric: **anchor descriptions**. Don't just say "1 = bad, 5 = great." Describe exactly what a 1, 3, and 5 look like with concrete examples.`,
    whyItMatters:
      "Scoring criteria are the bridge between 'I think this output is fine' and 'this output meets our quality bar.' In enterprise deployments, stakeholders need to agree on what quality means before launching. A well-written rubric aligns engineering, product, and domain experts on a single standard.",
    keyPrinciples: [
      "Choose the simplest scoring approach that captures what matters — binary if possible, component if needed",
      "Always write anchor descriptions — what does a 1, 3, and 5 actually look like?",
      "Score dimensions independently — an output can be accurate but too verbose",
      "Define a passing threshold before running the eval, not after seeing results",
    ],
  },
  example: {
    scenario:
      "You need a scoring rubric for evaluating incident response messages generated by an LLM for an IT operations team.",
    bad: {
      userMessage: `Score this incident response message on a scale of 1-5:

"There's a problem with the server. We're looking into it. Will update soon."`,
      assistantResponse: `Score: 3/5

The message acknowledges the issue but lacks specific details about the incident and timeline.`,
      annotations: [
        {
          text: "Score: 3/5",
          note: "What does a 3 mean? Without anchor descriptions, this is just a gut feeling disguised as a number.",
        },
        {
          text: "lacks specific details",
          note: "This feedback is subjective. Another reviewer might give it a 2 or a 4 for the same reasons.",
        },
      ],
    },
    good: {
      userMessage: `## Scoring Rubric: Incident Response Messages

**Dimension 1: Specificity** (does the message identify what's affected?)
- 1 — Vague: "there's an issue" with no specifics
- 3 — Partial: identifies the general area (e.g., "server issue") but not the specific service or impact
- 5 — Specific: names the affected service, scope of impact, and symptoms

**Dimension 2: Actionability** (does the reader know what's happening next?)
- 1 — No next steps: "we're looking into it"
- 3 — Vague timeline: "we'll update soon" or "working on a fix"
- 5 — Concrete plan: specific next step with a timeline (e.g., "deploying hotfix by 3pm PST")

**Dimension 3: Tone** (appropriate for the audience?)
- 1 — Alarming or dismissive
- 3 — Neutral but impersonal
- 5 — Professional, calm, and empathetic

**Passing threshold:** Average score ≥ 3.5 across all dimensions. No single dimension below 2.

---

**Scoring the example message:** "There's a problem with the server. We're looking into it. Will update soon."

- Specificity: 1 — says "problem" and "server" but no specific service, impact, or symptoms
- Actionability: 1 — "looking into it" and "update soon" give no concrete plan or timeline
- Tone: 3 — neutral, not alarming, but impersonal

**Average: 1.7 — FAIL** (below 3.5 threshold)`,
      assistantResponse: `The rubric clearly identifies that this message fails primarily on specificity and actionability. Here's an improved version:

"[INCIDENT] The payment processing service is experiencing elevated error rates (currently affecting ~15% of transactions). Our on-call engineer has identified the root cause as a database connection pool exhaustion. We're deploying a configuration fix and expect resolution within 30 minutes. Next update by 2:30pm PST."

Re-scoring:
- Specificity: 5 — names service, quantifies impact, identifies root cause
- Actionability: 5 — specific fix, timeline, and next update commitment
- Tone: 5 — professional, calm, factual

Average: 5.0 — PASS`,
      annotations: [
        {
          text: "Dimension 1: Specificity",
          note: "Each dimension measures one independent aspect of quality — you can be specific but have a bad tone.",
        },
        {
          text: "1 — Vague",
          note: "Anchor descriptions with concrete examples. Anyone reading this rubric would score a '1' the same way.",
        },
        {
          text: "Average score ≥ 3.5",
          note: "Threshold defined before scoring — no temptation to adjust it based on results.",
        },
        {
          text: "No single dimension below 2",
          note: "Floor constraint prevents a message from passing with great tone but zero useful information.",
        },
      ],
    },
  },
  exercise: {
    instructions: `**Your task:** Write a scoring rubric for a meeting notes summarizer.

The system takes raw meeting transcripts and generates concise summaries. Design a rubric with:
- At least 3 scoring dimensions (e.g., accuracy, completeness, conciseness)
- Anchor descriptions at 3 levels (e.g., 1, 3, and 5) for each dimension
- A passing threshold
- At least one constraint (e.g., minimum score on a critical dimension)

Write the rubric as a structured document with clear headings.`,
    starterCode: `## Scoring Rubric: Meeting Notes Summarizer

### Dimension 1: [Name]
- 1 — [description of poor performance]
- 3 — [description of acceptable performance]
- 5 — [description of excellent performance]

### Dimension 2: [Name]
- 1 — [description]
- 3 — [description]
- 5 — [description]

### Dimension 3: [Name]
- 1 — [description]
- 3 — [description]
- 5 — [description]

### Passing Threshold
[overall pass criteria and any per-dimension constraints]`,
    hints: [
      "Think about what makes a meeting summary useful: did it capture the key decisions? Is it the right length? Is it accurate?",
      "Anchor descriptions should be specific enough that two reviewers would assign the same score independently.",
      "Consider adding a 'no fabrication' floor constraint — a summary that invents decisions should auto-fail.",
      "A good threshold balances strictness with practicality — too strict and nothing passes, too loose and bad summaries ship.",
    ],
    validation: [
      {
        type: "min-length",
        value: 400,
        message:
          "Your rubric needs more detail — flesh out anchor descriptions for each dimension.",
      },
      {
        type: "regex",
        value: "(1|one).{1,80}(3|three).{1,80}(5|five)",
        message:
          "Include anchor descriptions at multiple levels (e.g., 1, 3, and 5) for your scoring scale.",
      },
      {
        type: "regex",
        value:
          "[Dd]imension|[Cc]ategor|[Aa]spect|[Cc]riter",
        message:
          "Define at least 3 separate scoring dimensions (e.g., accuracy, completeness, conciseness).",
      },
      {
        type: "regex",
        value: "[Tt]hreshold|[Pp]ass(ing|es)?|[Mm]inimum|[Ff]loor|≥|>=",
        message:
          "Define a passing threshold — what overall score means the output is acceptable?",
      },
      {
        type: "regex",
        value:
          "[Aa]ccura|[Cc]omplete|[Cc]oncis|[Ff]abrcat|[Hh]allucin|[Aa]ction [Ii]tem|[Dd]ecision",
        message:
          "Your dimensions should address key aspects of meeting summaries like accuracy, completeness, or action item capture.",
      },
    ],
    sampleSolution: `## Scoring Rubric: Meeting Notes Summarizer

### Dimension 1: Accuracy
Does the summary faithfully represent what was discussed and decided?

- 1 — Contains fabricated decisions or action items not in the transcript. Misattributes statements to wrong people.
- 3 — Captures the main topics accurately but may overstate certainty on tentative discussions or miss nuance (e.g., reports a "decision" that was actually a suggestion).
- 5 — Every stated fact, decision, and action item is directly supported by the transcript. Correctly distinguishes decisions from open questions.

### Dimension 2: Completeness
Does the summary capture all key outcomes?

- 1 — Misses major decisions or action items. A reader would leave without knowing critical outcomes.
- 3 — Captures the primary decision and most action items. May miss secondary topics or context that affected decisions.
- 5 — Captures all decisions, action items (with owners and deadlines), and open questions. Reader has full picture without reading the transcript.

### Dimension 3: Conciseness
Is the summary appropriately brief without losing substance?

- 1 — Either excessively verbose (restates discussion blow-by-blow) or so short it omits key information.
- 3 — Reasonable length but includes some unnecessary detail (e.g., "Sarah suggested X and then Marcus agreed and then they discussed..." instead of "Team decided X").
- 5 — Every sentence adds value. Uses bullet points or structured format. No filler phrases. Reads in under 2 minutes for a 1-hour meeting.

### Dimension 4: Structure
Is the output well-organized and easy to scan?

- 1 — Wall of text with no headings or grouping. Reader must read everything to find action items.
- 3 — Some structure (e.g., paragraphs by topic) but action items are buried in prose.
- 5 — Clear sections (Decisions, Action Items, Open Questions). Action items formatted with owner, task, and deadline. Easy to scan in 30 seconds.

### Passing Threshold
- **Overall:** Average score across all 4 dimensions ≥ 3.5
- **Accuracy floor:** Accuracy dimension must score ≥ 3. Any fabricated decision or action item is an automatic fail regardless of other scores.
- **Completeness floor:** Completeness must score ≥ 2. Missing a major decision is unacceptable even if the rest is well-written.`,
  },
};
